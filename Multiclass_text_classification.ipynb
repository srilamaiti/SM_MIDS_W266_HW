{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/SM_MIDS_W266_HW/blob/main/Multiclass_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-44Qz4-Xd1o"
      },
      "source": [
        "# Assignment 3: Fine tuning a multiclass classification BERT model\n",
        "\n",
        "**Description:** This assignment covers fine-tuning of a multiclass classification. You will compare two different types of solutions using BERT-based models. You should also be able to develop an intuition for:\n",
        "\n",
        "\n",
        "* Working with BERT\n",
        "* Using multiple models to focus on different sub-tasks\n",
        "* Different metrics to measure the effectiveness of your model\n",
        "\n",
        "\n",
        "\n",
        "The assignment notebook closely follows the lesson notebooks. We will use the 20 newsgroups dataset and will leverage some of the models, or part of the code, for our current investigation. \n",
        "\n",
        "**You are strongly encouraged to read through the entire notebook before answering any questions or writing any code.**\n",
        "\n",
        "The initial part of the notebook is purely setup. We will then generate our BERT model and see if and how we can improve it. \n",
        "\n",
        "Do not try to run this entire notebook on your GCP instance as the training of models requires a GPU to work in a timely fashion. This notebook should be run on a Google Colab leveraging a GPU. By default, when you open the notebook in Colab it will try to use a GPU. Total runtime of the entire notebook (with solutions and a Colab GPU) should be about 1h.\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2023-spring-main/blob/master/assignment/a3/Multiclass_text_classification.ipynb)\n",
        "\n",
        "The overall assignment structure is as follows:\n",
        "\n",
        "1. Setup\n",
        "  \n",
        "  1.1 Libraries & Helper Functions\n",
        "\n",
        "  1.2 Data Acquisition\n",
        "\n",
        "  1.3 Training/Test/Validation Sets for BERT-based models\n",
        "\n",
        "2. Classification with a fine tuned BERT model\n",
        "\n",
        "  2.1 Create the specified BERT model\n",
        "  \n",
        "  2.2 Fine tune the BERT model as directed\n",
        "\n",
        "  2.3 Examine the predictions with various metrics\n",
        "\n",
        "3. Classification using two stages\n",
        "\n",
        "  3.1 Relabel the data to group the often confused classes\n",
        "\n",
        "  3.2 Train the first stage model on the relabeled data\n",
        "\n",
        "  3.3 Separate the data for just the confused classes\n",
        "\n",
        "  3.4. Train the second stage model on the two classes\n",
        "\n",
        "  3.5. Combine and evaluate the predictions from the two stages\n",
        "\n",
        "4. Look at examples of misclassifications, see what might have changed\n",
        "\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**: \n",
        "\n",
        "* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the **answers** file as you did in a1 and a2.\n",
        "\n",
        "* **### YOUR CODE HERE** indicates that you are supposed to write code.\n",
        "\n",
        "* If you want to, you can run all of the cells in section 1 in bulk. This is setup work and no questions are in there. At the end of section 1 we will state all of the relevant variables that were defined and created in section 1.\n",
        "\n",
        "* **IMPORTANT NOTE:** Because the data we're using is downloaded each time we run section 1, a different split of train, validation, and test records is created.  This means that the accuracy, precision, recall, and F1 scores will change, although the delta will be small.  Please enter the values from your final run so that the answer values in your answers file correspond to the answer values in the outputs in your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK2xTV-Xisrl"
      },
      "source": [
        "### 1. Setup\n",
        "\n",
        "Lets get all our libraries and download and process our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "3gzOXaFdmgO8"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULcKJUjHmkZT",
        "outputId": "f1b4f40d-0dc9-48c3-c6c1-80626f198e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.8/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from pydot) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "aTq7Qjqbmkfv"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "-6E-xzDawK0Q"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "99yIOuqFXMwU"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "UWk_PZ9hmkk0"
      },
      "outputs": [],
      "source": [
        "def read_20newsgroups(test_size=0.1):\n",
        "  # download & load 20newsgroups dataset from sklearn's repos\n",
        "  dataset = fetch_20newsgroups(subset=\"all\", shuffle=True, remove=(\"headers\", \"footers\", \"quotes\"))\n",
        "  documents = dataset.data\n",
        "  labels = dataset.target\n",
        "  # split into training & testing a return data as well as label names\n",
        "  return train_test_split(documents, labels, test_size=test_size), dataset.target_names\n",
        "  \n",
        "# call the function\n",
        "(train_texts, test_texts, train_labels, test_labels), target_names = read_20newsgroups()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ocyl9dnb1OM"
      },
      "source": [
        "Take a look at the records.  We basically have a long string of text and an associated label.  That label is the Usenet group where the posting occured. The records are the raw text.  They vary significantly in size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Akxu2Umkpo",
        "outputId": "4e924358-36c3-4637-a92f-2c5765263b9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nAaron> Colossians 2:11-12 \"In him you were also circumcised, in the\\nAaron> putting off of the sinful nature, not with a circumcision done\\nAaron> by Christ, having been buried with him in baptism and raised\\nAaron> with him through your faith in the power of God, who raised him\\nAaron> from the dead.\"\\n\\nAaron> In baptism, we are raised to a new life in Christ (Romans 6:4)\\nAaron> through a personal faith in the power of God.  Our parent\\'s\\nAaron> faith cannot do this.  Do infants have faith?  Let\\'s look at\\nAaron> what the Bible has to say about it.\\n\\nYes, let do.  Try: \\n\\n\"And if anyone causes one of these little ones *who believes in me*\\nto sin, it would be better for him to be thrown into the sea\\nwith a large millstone tied around his neck.\"  Mark 9:42\\n\\n\"Let the little children come to me, and do not hinder them,\\nfor the kingdom of God belongs to such as these.  I tell you\\nthe truth, anyone who will not receive the kingdom of God\\nlike a little child will never enter it.\"\\n\\nThe Colossians passage does not make faith a requirement\\nfor baptism.  It merely says that in baptism we are born again,\\nregenerated, and resurrected through faith.  In the case\\nof an infant I would say that baptism works faith in the\\nheart of the infant--through the power of the word.\\n\\nThe Colossians passage does make baptism a spiritual circumcision.\\nCircumcision was the means by which a male infant was made\\na part of God\\'s covenant with Israel.  It was commanded to be\\nperformed on the eighth day.  The early church understood this,\\nand even debated whether baptism had to be performed on the\\neighth day, or if it could in fact be done earlier.\\n\\nAaron> Romans 10:16-17 \"But not all the Israelites accepted the good\\nAaron> news.  For Isaiah says, \\'Lord, who has believed our message?\\'\\nAaron> Consequently, faith comes from hearing the message, and the\\nAaron> message is heard through the word of Christ.\"\\n\\nAaron> So then we receive God\\'s gift of faith to us as we hear the\\nAaron> message of the gospel.  \\n\\nAnd the gospel is surely preached at any infant\\'s (or adult\\'s)\\nbaptism.  Indeed, in a very real sense, the sacraments are\\nthe Gospel made tangible.\\n\\nAaron> Faith is a possible response to hearing\\nAaron> God\\'s word preached.  Kids are not yet spiritually,\\nAaron> intellectually, or emotionally mature enough to respond to\\nAaron> God\\'s word.  \\n\\nHow do you know they are not yet mature enough to have faith?\\nDo you know this on the basis of God\\'s Word, or from your own\\nreason?\\n\\nFaith is also described as a gift from God, Ephesians 2:8,9. \\nHe gives faith to infants just as he gives it to adults, through\\nthe power of the gospel, Romans 1:6.\\n\\nAaron> If you read all of Ezekiel 18, you will see that God doesn\\'t\\nAaron> hold us guilty for anyone else\\'s sins.  So we can have no\\nAaron> original guilt from Adam.\\n\\nHere you show that you just don\\'t understand original sin--\\nyou are arguing against a straw man.\\nMaybe you\\'ve been talking to Catholics too much.  I don\\'t\\nknow.  But original sin does not consist of God\\'s imputation\\nof Adam\\'s guilt to us.  It consists of our inheritance of\\nAdam\\'s sinful nature.  It is actual sin.  See for example,\\nthe Augsburg Confession, Article II, and the Apology of the\\nAugsburg Confession, Article II, and, for extra credit,\\nJohn Knox\\'s `The Scots confession\\', Article III.\\n\\nAaron> Now then that we have a little more background as to why\\nAaron> original sin is not Biblical, let\\'s look at some of the\\nAaron> scriptures used to support it.\\n\\nAaron> Romans 5:12 \"Therefore, just as sin entered the world through\\nAaron> one man, and death through sin, and in this way death came to\\nAaron> all men, because all sinned--\"\\n\\n\\nAsk yourself this question.  \"Do infants ever die?\"  Then ask\\nyourself, \"If infant baptism is not valid, then where was the\\nChristian Church during all the centuries when almost all \\nof the baptisms were performed on infants?  Were Luther, Melancthon,\\nCalvin, Zwingli, Hus, Knox, Andrae, and Chemnitz Christians?\\n\\n\\nAaron> Psalm 51:5 \"Surely I was sinful at birth, sinful from the time\\nAaron> my mother conceived me.\"\\n\\nAaron> This whole Psalm is a wonderful example of how we should humble\\nAaron> ourselves before God in repentance for sinning.  David himself\\nAaron> was a man after God\\'s own heart and wrote the Psalm after\\nAaron> committing adultry with Bathsheba and murdering her husband.\\nAaron> All that David is saying here is that he can\\'t remember a time\\nAaron> when he wasn\\'t sinful.  He is humbling himself before God by\\nAaron> confessing his sinfulness.  His saying that he was sinful at\\nAaron> birth is a hyperbole.  The Bible, being inspired by God, isn\\'t\\nAaron> limited to a literal interpetation, but also uses figures of\\nAaron> speech as did Jesus (John 16:25).  For another example of\\nAaron> hyperbole, see Luke 14:26.\\n\\nWho are you to say what is literal and what is not?  Is a literal\\ninterpretation manifestly absurd in Psalm 51 by reason of direct\\ncontradiction with a clear passage from the Word of God?\\n\\nYou might also compare Genesis 8:21, \"The LORD smelled the\\npleasing aroma and said in his heart, `Never again will I curse\\nthe ground because of man, even though every inclination of\\nhis heart is evil from childhood....\"\\n\\nAaron> We see\\nAaron> that he did grow and become wiser in Luke 2:40 and 2:52.  The\\nAaron> implication is that Jesus did wrong things as a child before he\\nAaron> knew to choose right over wrong.\\n\\nYou are a long way from proving this (rather monstrous) assertion.\\nAll you can say is that Jesus grew in wisdom and in stature.  A\\nconclusion that he did wrong as a child is based on an extrapolation\\nof reason, not on a direct revelation in Scripture.',\n",
              " \"\\n\\nThis is silly. Is Unix a mature OS? Depends on who you ask, and how\\nyou define mature. System 7 is, if anything, less mature than Windows 3.1.\\n\\n\\n\\nSo why do you need something like BeHierarchic to create groups under\\nthe Apple Menu? Everyone knows that Apple Menu Items are a ripoff of\\nthe Program Manager. If you want a hierarchic program launcher there\\nare lots available.\\n\\n\\nAnd this is easy on a Mac? Give me a break. Having spent hours moving\\nSystem Extensions around and restarting the Mac to see why a certain\\napp crashes all the time, I find this laughable.\\n\\n\\nOh great. Ever hear of aliases? Wonder why Apple implemented them.\\n\\n\\nEh?? I don't follow.\\n\\n\\nWhy is it that I find the Mac desktop incredibly annoying whenever I\\nuse it? \\n\\n\\nYeah right. You post flame bait, yet ask for no flames. \\n\\n\\n\\n\\n\\n\\n\\n\\n-- \"]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "train_texts[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eKIgSBdcHy9"
      },
      "source": [
        "Notice the \"labels\" are just integers that are an offset into the list of target names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRnu9CSQnMNN",
        "outputId": "a0fc049e-3b51-42e0-b1b9-347054708be9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15,  2])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "train_labels[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ksEnmbcWBt"
      },
      "source": [
        "The variable ''target_names'' stores all of the names of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvNAnCstx-3S",
        "outputId": "522fcb8e-f07d-44c6-deec-8001615694d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ],
      "source": [
        "print(target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1SmEVYOcwP6"
      },
      "source": [
        "We already have a test set and a train set.  Let's explicitly set aside part of our training set for validation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "H1toWirQAAZ8"
      },
      "outputs": [],
      "source": [
        "len(train_texts)\n",
        "valid_texts = train_texts[16000:]\n",
        "valid_labels = train_labels[16000:]\n",
        "train_texts = train_texts[:16000]\n",
        "train_labels = train_labels[:16000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBMiGVwHc76-"
      },
      "source": [
        "The validation set will always have 961 records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzuXsJjwAAOr",
        "outputId": "7ac2dac7-95f6-40e1-811a-3f8c262a615b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "961"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "len(valid_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBy5EKYwdBcZ"
      },
      "source": [
        "The training set will always have 16000 records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMVgiD8SCHUk",
        "outputId": "1f10f0e6-effe-4f77-c3c4-4a436161b240"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "len(train_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGVOwMBKu2OW"
      },
      "source": [
        "**NOTE:** Each time you rerun the data you will draw a *different* set of train and test documents even though the numbers 961 and 16000 will always be the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "mFISqjoM335H"
      },
      "outputs": [],
      "source": [
        "#get the labels in a needed data format for validation\n",
        "npvalid_labels = np.asarray(valid_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3T5uytRdK7I"
      },
      "source": [
        "* train_texts - an array of text strings for training\n",
        "* test_texts - an array of text strings for testing \n",
        "* valid texts - an array of text strings for validation\n",
        "* train_labels - an array of integers representing the labels associated with train_texts\n",
        "* test_labels - an array of integers representing the labels associated with test_texts\n",
        "* valid_labels - an array of integers representing the labels associated with valid_texts\n",
        "* target_names - an array of label strings that correspond to the integers in the *_labels arrays\n",
        "\n",
        "### 2. Classification with a fine tuned BERT model\n",
        "\n",
        "Let's pick our BERT model.  We'll start with the base BERT model and we'll use the cased version since our data has capital and lower case letters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "zjFWaM2ZnMIC"
      },
      "outputs": [],
      "source": [
        "#make it easier to use a variety of BERT subword models\n",
        "model_checkpoint = 'bert-base-cased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7gV_GUdn9Ck",
        "outputId": "653dec65-5d9c-486b-8f4b-e4095508b84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
        "bert_model = TFBertModel.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJGkIHSHdilH"
      },
      "source": [
        "We're setting our maximum training record length to 200.  BERT models can handle more and after you've completed the assignment you're welcome to try larger and small sized records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "h_59AaVznMCV"
      },
      "outputs": [],
      "source": [
        "max_length = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6d54_bqd58L"
      },
      "source": [
        "Now we'll tokenize our three data slices.  This will take a minute or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgv0nftinL6z"
      },
      "outputs": [],
      "source": [
        "# tokenize the dataset, truncate when passed `max_length`, \n",
        "# and pad with 0's when less than `max_length` and return a tf Tensor\n",
        "train_encodings = bert_tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "valid_encodings = bert_tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "test_encodings = bert_tokenizer(test_texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gNS0Fi0emN8"
      },
      "source": [
        "Notice our input_ids for the first training record and their padding. The train_encodings also includes an array of token_type_ids and an attention_mask array. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFEXxgAmnLve"
      },
      "outputs": [],
      "source": [
        "train_encodings.input_ids[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23lwfDw2T6gI"
      },
      "source": [
        "Write a function to create this multiclass bert model.\n",
        "\n",
        "Keep in mind the following:\n",
        "* Each record can have one of n labels where n = the size of target_names.\n",
        "* We'll still want a hidden size layer of size 201\n",
        "* We'll want our hidden layer to make use of the **pooler output** from BERT\n",
        "* We'll also want to use dropout\n",
        "* Our classification layer will need to be appropriately sized and use the correct non-linearity for a multi-class problem.\n",
        "* Since we have multiple labels we can no longer use binary cross entropy.  Instead we need to change our loss metric to a categorical cross entropy.  Which of the two categorical cross entropy metrics will work best here? \n",
        "* Make sure that training affects **all** of the layers in BERT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtWMrLi4tIun"
      },
      "outputs": [],
      "source": [
        "def create_bert_multiclass_model(checkpoint = model_checkpoint,\n",
        "                                 num_classes = 20,\n",
        "                                 hidden_size = 201, \n",
        "                                 dropout=0.3,\n",
        "                                 learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the Pooler Output for classification purposes.\n",
        "    \"\"\"\n",
        "    # Add trainable to True\n",
        "    bert_model = TFBertModel.from_pretrained(checkpoint)                \n",
        "    bert_model.trainable = True                              \n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}      \n",
        "\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    pooler_token = bert_out[1]\n",
        "    #cls_token = bert_out[0][:, 0, :]\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(pooler_token)\n",
        "\n",
        "\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
        "\n",
        "\n",
        "    classification = tf.keras.layers.Dense(num_classes, \n",
        "                                           activation='softmax',\n",
        "                                           name='classification_layer')(hidden)\n",
        "    \n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
        "    \n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 #loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                                 loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                                 metrics='accuracy')\n",
        "    \n",
        "    ### END YOUR CODE\n",
        "    return classification_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR-Id158tIlH"
      },
      "outputs": [],
      "source": [
        "pooler_bert_model = create_bert_multiclass_model(checkpoint=model_checkpoint, num_classes=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMxpO16-tIaE"
      },
      "outputs": [],
      "source": [
        "pooler_bert_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5chyZZnHdlf9"
      },
      "source": [
        "**QUESTION:** 2.1 How many trainable parameters are in your dense hidden layer?\n",
        "**154,569**\n",
        "\n",
        "**QUESTION:** 2.2 How many trainable parameters are in your classification layer? **4040**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe3tTWX6FVZL"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(pooler_bert_model, show_shapes=False, show_dtype=False, show_layer_names=True, dpi=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLLjb3mOtICy"
      },
      "outputs": [],
      "source": [
        "#It takes 10 to 14 minutes to complete an epoch when using a GPU\n",
        "pooler_bert_model_history = pooler_bert_model.fit([train_encodings.input_ids, \n",
        "                                                   train_encodings.token_type_ids, \n",
        "                                                   train_encodings.attention_mask\n",
        "                                                  ], \n",
        "                                                  train_labels,   \n",
        "                                                  validation_data=([valid_encodings.input_ids, \n",
        "                                                                    valid_encodings.token_type_ids, \n",
        "                                                                    valid_encodings.attention_mask\n",
        "                                                                   ], \n",
        "                                                  npvalid_labels),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdHlOd4LJUOH"
      },
      "source": [
        "Now we need to run evaluate against our fine-tuned model.  This will give us an overall accuracy based on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPQn2tJPEGdU"
      },
      "outputs": [],
      "source": [
        "#batch 8, ML=201\n",
        "score = pooler_bert_model.evaluate([test_encodings.input_ids, \n",
        "                                    test_encodings.token_type_ids, \n",
        "                                    test_encodings.attention_mask\n",
        "                                   ], \n",
        "                                   test_labels) \n",
        "\n",
        "print('Test loss:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CnyZsXLfLJt"
      },
      "source": [
        "**QUESTION:** 2.3 What is the Test accuracy score you get from your model? (Just copy and paste the value into the answers sheet and round to five significant digits.) **0.68700**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS7Gp_IYEiJ_"
      },
      "outputs": [],
      "source": [
        "#run predict for the first three elements in the test data set\n",
        "predictions = pooler_bert_model.predict([test_encodings.input_ids[:3], \n",
        "                                         test_encodings.token_type_ids[:3], \n",
        "                                         test_encodings.attention_mask[:3]\n",
        "                                        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymGM8QnpchnC"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ahMJ-zwEiCs"
      },
      "outputs": [],
      "source": [
        "#run and capture all predictions from our test set using model.predict\n",
        "### YOUR CODE HERE\n",
        "predictions_model1 = pooler_bert_model.predict([test_encodings.input_ids, \n",
        "                                                test_encodings.token_type_ids, \n",
        "                                                test_encodings.attention_mask\n",
        "                                               ])\n",
        "### END YOUR CODE\n",
        "\n",
        "#now we need to get the highest probability in the distribution for each prediction\n",
        "#and store that in a tf.Tensor\n",
        "predictions_model1 = tf.argmax(predictions_model1, axis=-1)\n",
        "predictions_model1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGI8oA6fMWTI"
      },
      "source": [
        "There are two ways to see what's going on with our classifier.  Overall accuracy is interesting but it can be misleading.  We need to make sure that each of our categories' prediction performance is operating at an equal or higher level than the overall.\n",
        "\n",
        "Here we'll use the classification report from scikit learn.  It expects two inputs as arrays.  One is the ground truth (y_true) and the other is the associated prediction (y_pred).  This is based on gethering all the predictions from our our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPqioIbh2XIE"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test_labels, predictions_model1.numpy(), target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_report(test_labels, predictions_model1.numpy(), target_names=target_names, output_dict = True)"
      ],
      "metadata": {
        "id": "nN-5cQVW_hVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_orig = classification_report(test_labels, \n",
        "                                    predictions_model1.numpy(), \n",
        "                                    target_names=target_names, \n",
        "                                    output_dict = True, \n",
        "                                    zero_division=False\n",
        "                                  )"
      ],
      "metadata": {
        "id": "7Y-tfBYzti9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddMMQ2vBOJKj"
      },
      "source": [
        "**QUESTION:** 2.4 What is the key difference between the macro average F1 score and the weighted average F1 score?\n",
        "**In Macro-average F1 score, all classes contribute equally to the final average value. This does not work for imbalanced data set. For weighted average F1 score considers each representative class's weighted contribution towards the average.**\n",
        "\n",
        "**QUESTION:** 2.5 What is the macro average F1 score you get from the classification report?\n",
        "**0.65974**\n",
        "\n",
        "Now we'll generate another very valuable visualization of what's happening with our classifier -- a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udwdL6iKEh7b"
      },
      "outputs": [],
      "source": [
        "cm_orig = tf.math.confusion_matrix(test_labels, predictions_model1)\n",
        "cm_orig[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.shape, cm_orig.shape"
      ],
      "metadata": {
        "id": "Zv-E5X8PNQ63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = cm_orig /cm_orig.numpy().sum(axis=1)[:, tf.newaxis]\n",
        "cm[0]"
      ],
      "metadata": {
        "id": "u9_GtH_tdwfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_orig[-1]"
      ],
      "metadata": {
        "id": "EPXkiD5nJ9Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm[-1]"
      ],
      "metadata": {
        "id": "2VsFTdNfKCTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9YL2AlQOY4Y"
      },
      "source": [
        "And now we'll display it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh0Bxg-8EhzZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "sns.heatmap(\n",
        "    cm, annot=True,\n",
        "    xticklabels=target_names,\n",
        "    yticklabels=target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OKhCGVDOgdl"
      },
      "source": [
        "### 3. Classification using two stages\n",
        "\n",
        "Okay, not bad.  As you can see, some categories are easier to distinguish than others. Look for the class with the lowest F1 score (it should be the one at the bottom of the list). In the confusion matrix, which other class is that one being mistaken for most often?\n",
        "\n",
        "You might notice that the categories in this dataset are somewhat heirarchical. There are more obvious differences between groups of news categories (e.g. computers vs recreation) and then subtler differences between categories within those groups (e.g. PC vs Mac, within computers).\n",
        "\n",
        "When this happens, one idea is to train a series of models, to first separate out the more obvious groups of classes, and then use more specialized sub-models to classify only a subset of the classes. Let's try that here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0qA4QBstsin"
      },
      "source": [
        "#### Step 1: New model with 19 classes\n",
        "\n",
        "For simplicity, we'll just combine two categories in our first step. We'll replace the label of the last class with the label of the class it's most often mistaken for. (That way, we'll have labels from 0 to 18 instead of 0 to 19, and don't have to renumber everything, though you would have to if you group them more.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate precision scores\n",
        "classification_precision_scores = precision_score(test_labels, predictions_model1.numpy(), average = None)\n",
        "y_pos = np.arange(len(classification_precision_scores))\n",
        "\n",
        "plt.bar(y_pos, classification_precision_scores, align = 'center', alpha = 0.5)\n",
        "plt.xticks(y_pos, set(test_labels), rotation = 90)\n",
        "plt.ylabel('Precision Score')\n",
        "plt.title('Precision scores per class')\n",
        "\n",
        "print(\"Off the positive predictions, % of the right predictions per class\")\n",
        "for prec, lbl in list(zip(classification_precision_scores, set(test_labels))):\n",
        "    print(f\"{lbl} class has precision : {prec}\")\n",
        "plt.show(block = False)\n",
        "\n",
        "# Calculate recall scores\n",
        "classification_recall_scores = recall_score(test_labels, predictions_model1.numpy(), average = None)\n",
        "y_pos = np.arange(len(classification_precision_scores))\n",
        "\n",
        "plt.bar(y_pos, classification_recall_scores, align = 'center', alpha = 0.5)\n",
        "plt.xticks(y_pos, set(test_labels), rotation = 90)\n",
        "plt.ylabel('Recall Score')\n",
        "plt.title('Recall scores per class')\n",
        "\n",
        "print(\"Off the positive cases, % of the right predictions per class\")\n",
        "for prec, lbl in list(zip(classification_precision_scores, set(test_labels))):\n",
        "    print(f\"{lbl} class has recall : {prec}\")\n",
        "plt.show(block = False)\n",
        "\n",
        "# Calculate F1 scores\n",
        "classification_f1_scores = f1_score(test_labels, predictions_model1.numpy(), average = None)\n",
        "y_pos = np.arange(len(classification_f1_scores))\n",
        "\n",
        "plt.bar(y_pos, classification_f1_scores, align = 'center', alpha = 0.5)\n",
        "plt.xticks(y_pos, set(test_labels), rotation = 90)\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 scores per class')\n",
        "\n",
        "print(\"F1 scores per class\")\n",
        "for prec, lbl in list(zip(classification_f1_scores, set(test_labels))):\n",
        "    print(f\"{lbl} class has f1 score : {prec}\")\n",
        "plt.show(block = False)"
      ],
      "metadata": {
        "id": "z9EbRp_A4_yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(cm[-1]), np.argmax(cm[-1]), np.argmin(cm[-1]), np.min(cm[-1])"
      ],
      "metadata": {
        "id": "XoXbVviOV5Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0rAv6nLmrwN"
      },
      "outputs": [],
      "source": [
        "label_to_replace = 19\n",
        "\n",
        "# label_to_replace_with = ...\n",
        "### YOUR CODE HERE\n",
        "label_to_replace_with = np.argmax(cm[-1])\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "train_labels_19class = train_labels.copy()\n",
        "train_labels_19class[train_labels_19class == label_to_replace] = label_to_replace_with\n",
        "\n",
        "valid_labels_19class = npvalid_labels.copy()\n",
        "valid_labels_19class[valid_labels_19class == label_to_replace] = label_to_replace_with\n",
        "\n",
        "test_labels_19class = test_labels.copy()\n",
        "test_labels_19class[test_labels_19class == label_to_replace] = label_to_replace_with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSQd_UDQhmQd"
      },
      "source": [
        "Now let's create a new model with the same architecture, but to predict probabilities for 19 classes instead of 20. We're using all of the data in this first step, so we'll use the encodings we already preprocessed as inputs, but use the new labels that only have 19 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7hR7ahmmrYL"
      },
      "outputs": [],
      "source": [
        "bert_model_19class = create_bert_multiclass_model(checkpoint = model_checkpoint, num_classes=19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4Qsai6UmrOj"
      },
      "outputs": [],
      "source": [
        "bert_model_19class_history = bert_model_19class.fit([train_encodings.input_ids, \n",
        "                                                     train_encodings.token_type_ids, \n",
        "                                                     train_encodings.attention_mask\n",
        "                                                    ], \n",
        "                                                    train_labels_19class,   \n",
        "                                                    validation_data=([valid_encodings.input_ids, \n",
        "                                                                      valid_encodings.token_type_ids, \n",
        "                                                                      valid_encodings.attention_mask\n",
        "                                                                     ],\n",
        "                                                                     valid_labels_19class),    \n",
        "                                                    batch_size=8,\n",
        "                                                    epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkW9hMq9g9dV"
      },
      "outputs": [],
      "source": [
        "#Evaluate the fine tuned 19-class model against the test data with 19-class labels\n",
        "### YOUR CODE HERE\n",
        "score_19class = bert_model_19class.evaluate([test_encodings.input_ids, \n",
        "                                             test_encodings.token_type_ids, \n",
        "                                             test_encodings.attention_mask\n",
        "                                            ], \n",
        "                                            test_labels_19class) \n",
        "\n",
        "print('Test loss:', score_19class[0]) \n",
        "print('Test accuracy:', score_19class[1])\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPaUdul4hlp0"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        "3.1 What is the test accuracy you get when you run the new first stage model with only 19 classes?\n",
        "**0.685411**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrWtaXSug9UR"
      },
      "outputs": [],
      "source": [
        "#run and capture all the predictions from the 19 class data\n",
        "### YOUR CODE HERE \n",
        "predictions_19class = bert_model_19class.predict([test_encodings.input_ids, \n",
        "                                                  test_encodings.token_type_ids, \n",
        "                                                  test_encodings.attention_mask\n",
        "                                                 ]\n",
        "                                                )\n",
        "predictions_19class = tf.argmax(predictions_19class, axis=-1)\n",
        "### END YOUR CODE\n",
        "predictions_19class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_19class"
      ],
      "metadata": {
        "id": "Kj8j_rie1Rc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nAlDV7K8T4q"
      },
      "outputs": [],
      "source": [
        "# This will take columns before label_to_replace_with : :label_to_replace_with\n",
        "# This will take columns from next column of label_to_replace_with all the way till one column before the last column, which column to replace : label_to_replace_with+1:19              \n",
        "target_names_19class = target_names[:label_to_replace_with] \\\n",
        "                     + ['** COMBINED CLASS **'] \\\n",
        "                     + target_names[label_to_replace_with+1:19] \n",
        "print(classification_report(test_labels_19class, predictions_19class,\n",
        "                            target_names=target_names_19class))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_report(test_labels_19class, \n",
        "                      predictions_19class,\n",
        "                      target_names=target_names_19class, \n",
        "                      output_dict=True\n",
        "                     )"
      ],
      "metadata": {
        "id": "HFY7o6leWRaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_19cls = classification_report(test_labels_19class, \n",
        "                                     predictions_19class,target_names=target_names_19class, \n",
        "                                     output_dict=True, \n",
        "                                     zero_division=False\n",
        "                                    )"
      ],
      "metadata": {
        "id": "RhGH5NK2txXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = tf.math.confusion_matrix(test_labels_19class, predictions_19class)\n",
        "cm = cm/cm.numpy().sum(axis=1)[:, tf.newaxis]\n",
        "plt.figure(figsize=(20,7))\n",
        "sns.heatmap(\n",
        "    cm, annot=True,\n",
        "    xticklabels=target_names_19class,\n",
        "    yticklabels=target_names_19class)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")"
      ],
      "metadata": {
        "id": "U6uPHk19mNPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh6nKilF8V7b"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        "3.2 What is the F1 score you get for the combined class when you run the new first stage model with only 19 classes?\n",
        "**F1 score is 0.69872.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTZcBibxR153"
      },
      "source": [
        "#### Step 2: New model with only the two classes combined in step one\n",
        "\n",
        "Now, our first stage model is able to determine which text is one of the two often confused classes, but we need to train a more specific model to distinguish between just these two classes. Ideally, this model will only focus on the more subtle differences between these two news categories, since it doesn't have to learn everything else about the other categories.\n",
        "\n",
        "For this model, we're only going to train using the text examples that are one of the two confused categories. We'll keep the encodings we already tokenized, so we need to separate out the input_ids, token_type_ids, and attention_mask for just the rows that have one of these two labels."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_replace, label_to_replace_with"
      ],
      "metadata": {
        "id": "nqqK1IlhcTLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4TmPG6B1mz0"
      },
      "outputs": [],
      "source": [
        "train_mask_2class = (train_labels_19class == label_to_replace_with)\n",
        "train_encodings_2class = {'input_ids': train_encodings.input_ids[train_mask_2class],\n",
        "                          'token_type_ids': train_encodings.token_type_ids[train_mask_2class],\n",
        "                          'attention_mask': train_encodings.attention_mask[train_mask_2class]}\n",
        "train_labels_2class = train_labels.copy()[train_mask_2class]\n",
        "train_labels_2class = (train_labels_2class == label_to_replace_with).astype(int)\n",
        "\n",
        "valid_mask_2class = (valid_labels_19class == label_to_replace_with)\n",
        "valid_encodings_2class = {'input_ids': valid_encodings.input_ids[valid_mask_2class],\n",
        "                          'token_type_ids': valid_encodings.token_type_ids[valid_mask_2class],\n",
        "                          'attention_mask': valid_encodings.attention_mask[valid_mask_2class]}\n",
        "valid_labels_2class = npvalid_labels.copy()[valid_mask_2class]\n",
        "valid_labels_2class = (valid_labels_2class == label_to_replace_with).astype(int)\n",
        "\n",
        "test_mask_2class = (test_labels_19class == label_to_replace_with)\n",
        "test_encodings_2class = {'input_ids': test_encodings.input_ids[test_mask_2class],\n",
        "                          'token_type_ids': test_encodings.token_type_ids[test_mask_2class],\n",
        "                          'attention_mask': test_encodings.attention_mask[test_mask_2class]}\n",
        "test_labels_2class = test_labels.copy()[test_mask_2class]\n",
        "test_labels_2class = (test_labels_2class == label_to_replace_with).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwTzhDVp9YTT"
      },
      "outputs": [],
      "source": [
        "train_labels_2class.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O76HzIGh9dct"
      },
      "outputs": [],
      "source": [
        "train_labels_2class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c7M7sFCucU8"
      },
      "source": [
        "Create and train a new model with the same architecture as before, except that it only predicts two classes. (Note that we could change this to a binary prediction model, but we'll keep it multiclass for consistency here.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNe5zVOflttk"
      },
      "outputs": [],
      "source": [
        "bert_model_2class = create_bert_multiclass_model(checkpoint=model_checkpoint, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F5DM3zGlvSd"
      },
      "outputs": [],
      "source": [
        "bert_model_2class_history = bert_model_2class.fit([train_encodings_2class['input_ids'],\n",
        "                                                   train_encodings_2class['token_type_ids'],\n",
        "                                                   train_encodings_2class['attention_mask']], \n",
        "                                                  train_labels_2class,   \n",
        "                                                  validation_data=([valid_encodings_2class['input_ids'],\n",
        "                                                                    valid_encodings_2class['token_type_ids'],\n",
        "                                                                    valid_encodings_2class['attention_mask']],\n",
        "                                                                   valid_labels_2class\n",
        "                                                                  ),    \n",
        "                                                  batch_size=8, \n",
        "                                                  epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNEUeufxp1g5"
      },
      "outputs": [],
      "source": [
        "#Evaluate the two-class model against the two-class test set.\n",
        "### YOUR CODE HERE\n",
        "score_2class = bert_model_2class.evaluate([test_encodings_2class['input_ids'], \n",
        "                                           test_encodings_2class['token_type_ids'], \n",
        "                                           test_encodings_2class['attention_mask']\n",
        "                                          ], \n",
        "                                          test_labels_2class) \n",
        "\n",
        "### END YOUR CODE\n",
        "print('Test loss:', score_2class[0]) \n",
        "print('Test accuracy:', score_2class[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPuN8_gaESJZ"
      },
      "outputs": [],
      "source": [
        "#run and capture all the predictions from the 2-class test data\n",
        "### YOUR CODE HERE \n",
        "predictions_2class = bert_model_2class.predict([test_encodings_2class['input_ids'], \n",
        "                                                test_encodings_2class['token_type_ids'], \n",
        "                                                test_encodings_2class['attention_mask']\n",
        "                                               ]\n",
        "                                              )\n",
        "predictions_2class = tf.argmax(predictions_2class, axis=-1)\n",
        "### END YOUR CODE\n",
        "predictions_2class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEnHd3ijESPH"
      },
      "outputs": [],
      "source": [
        "# Run the sklearn classification_report again with the 2-class predictions\n",
        "### YOUR CODE HERE\n",
        "target_names_2class = [target_names[label_to_replace_with]] + [target_names[label_to_replace]]\n",
        "print(classification_report(test_labels_2class, predictions_2class.numpy(),target_names=target_names_2class))\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification_report(test_labels_2class, \n",
        "                      predictions_2class.numpy(),\n",
        "                      target_names=target_names_2class, \n",
        "                      output_dict=True)"
      ],
      "metadata": {
        "id": "CLrASyNXe4kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_2cls = classification_report(test_labels_2class, \n",
        "                                    predictions_2class,\n",
        "                                    target_names=target_names_2class, \n",
        "                                    output_dict=True, \n",
        "                                    zero_division=False\n",
        "                                   )"
      ],
      "metadata": {
        "id": "mRY95gXHt74C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTDW3JBut1P"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        "3.3 What is the macro average F1 score you get when you run the new second stage model with only 2 classes?\n",
        "**Macro average F1 score 0.75454.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90DMNd_Nuzgz"
      },
      "source": [
        "#### Step 3: Combine the predicted labels from the two steps\n",
        "\n",
        "To combine our models into two steps, start with the predictions from the first step. Keep all predicted labels except the ones with a predicted value of label_to_replace_with (the label we gave to both of the confused classes in the first step).\n",
        "\n",
        "Wherever the first model predicted the combined category, we'll replace the predictions with the label from the second model. If we used these models in inference, we'd only send an example to the second model if the first model predicted that it was from the combined class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmSd5B8M-YPh"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "predictions_2class = predictions_2class.numpy()\n",
        "predictions_2class[predictions_2class == 0] = label_to_replace\n",
        "predictions_2class[predictions_2class == 1] = label_to_replace_with\n",
        "\n",
        "predictions_2steps = predictions_19class.numpy()\n",
        "predictions_2steps[test_mask_2class] = predictions_2class\n",
        "\n",
        "predictions_2steps\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czf7y8Ha5hu-"
      },
      "outputs": [],
      "source": [
        "test_mark_2class_step3 = (predictions_19class.numpy() == label_to_replace_with)\n",
        "test_encodings_2class_step3 = {'input_ids': test_encodings.input_ids[test_mark_2class_step3],\n",
        "                               'token_type_ids': test_encodings.token_type_ids[test_mark_2class_step3],\n",
        "                               'attention_mask': test_encodings.attention_mask[test_mark_2class_step3]}\n",
        "\n",
        "# Running those examples through the step 2 model and save the predictions\n",
        "predictions_2class_step3 = bert_model_2class.predict([test_encodings_2class_step3['input_ids'],\n",
        "                                                      test_encodings_2class_step3['token_type_ids'],\n",
        "                                                      test_encodings_2class_step3['attention_mask']],)\n",
        "predictions_2class_step3 = tf.argmax(predictions_2class_step3, axis=-1)\n",
        "\n",
        "# Replace the step 2 model's predicted labels with the original values from 20 class dataset\n",
        "predictions_2class_step3 = predictions_2class_step3.numpy()\n",
        "predictions_2class_step3[predictions_2class_step3 == 0] = label_to_replace\n",
        "predictions_2class_step3[predictions_2class_step3 == 1] = label_to_replace_with\n",
        "\n",
        "# Combine the labels from bothb steps for the full dataset\n",
        "predictions_2steps_step3 = predictions_19class.numpy()\n",
        "predictions_2steps_step3[test_mark_2class_step3] = predictions_2class_step3\n",
        "\n",
        "predictions_2steps_step3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxUBa20mv0Iq"
      },
      "source": [
        "Now let's look at the classification report and confusion matrix, using the combined predictions from our two step model (compared to the original labels). Did the overall results get better?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_labels, predictions_2steps_step3, target_names=target_names))"
      ],
      "metadata": {
        "id": "1cKDRON5HjiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbtOD45X6QgO"
      },
      "outputs": [],
      "source": [
        "# Run the sklearn classification_report with all 20 classes from the 2-step predictions\n",
        "### YOUR CODE HERE\n",
        "classification_report(test_labels, predictions_2steps_step3, target_names=target_names, output_dict=True)\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_combined = classification_report(test_labels, predictions_2steps_step3, target_names=target_names, output_dict=True)"
      ],
      "metadata": {
        "id": "AnEDWi3DuTr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_replace, label_to_replace_with"
      ],
      "metadata": {
        "id": "Dl9lcUEcHPS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoxOfneGDG60"
      },
      "outputs": [],
      "source": [
        "cm = tf.math.confusion_matrix(test_labels, predictions_2steps_step3)\n",
        "cm = cm/cm.numpy().sum(axis=1)[:, tf.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of7FDh3CDHGy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,7))\n",
        "sns.heatmap(\n",
        "    cm, annot=True,\n",
        "    xticklabels=target_names,\n",
        "    yticklabels=target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jXBmgS2iXDI"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "3.4 What is the macro average F1 score you get from the combined two-step model? \n",
        "\n",
        "3.5 What is the difference in points between the macro weighted F1 score for the original model and the combined two-step model? \n",
        "\n",
        "3.6 What is the new F1 score for the last category (i.e. label_to_replace, the one that had the lowest F1 score in the original model)? \n",
        "\n",
        "3.7 What is the new F1 score for the other category that you combined with the last category in the two-step model (i.e. label_to_replace_with)? \n",
        "\n",
        "3.8 Which metric (precision or recall) is now lower for the other category (i.e. label_to_replace_with)? "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4 What is the macro average F1 score you get from the combined two-step model? **0.66160**"
      ],
      "metadata": {
        "id": "qwuhRCkMwweg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_combined['macro avg']['f1-score']"
      ],
      "metadata": {
        "id": "lWb9s2Rww0Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.5 What is the difference in points between the macro weighted F1 score for the original model and the combined two-step model? **0.00186**"
      ],
      "metadata": {
        "id": "6pemPZL_rBVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_combined['macro avg']['f1-score'] - report_orig['macro avg']['f1-score']"
      ],
      "metadata": {
        "id": "WG87DAvAue7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.6 What is the new F1 score for the last category (i.e. label_to_replace, the one that had the lowest F1 score in the original model)? **0.18750**"
      ],
      "metadata": {
        "id": "jJFhef3Xrf-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_replace, target_names[label_to_replace]"
      ],
      "metadata": {
        "id": "B2LCrclmr-Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_combined[target_names[label_to_replace]]['f1-score']"
      ],
      "metadata": {
        "id": "jOsm7Gucrreo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7 What is the new F1 score for the other category that you combined with the last category in the two-step model (i.e. label_to_replace_with)? **0.71296**"
      ],
      "metadata": {
        "id": "UcgeoD9Zu3Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_replace_with, target_names[label_to_replace_with]"
      ],
      "metadata": {
        "id": "chq5yj2xvDfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_combined[target_names[label_to_replace_with]]['f1-score']"
      ],
      "metadata": {
        "id": "CzXIAfA5u5li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.8 Which metric (precision or recall) is now lower for the other category (i.e. label_to_replace_with)? **precision**"
      ],
      "metadata": {
        "id": "8ldrSVzOvL4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Precision in 2step model : {report_combined[target_names[label_to_replace_with]]['precision']}\")\n",
        "print(f\"Recall in 2step model : {report_combined[target_names[label_to_replace_with]]['recall']}\")\n",
        "\n",
        "print(f\"Precision in original model : {report_orig[target_names[label_to_replace_with]]['precision']}\")\n",
        "print(f\"Recall in original model : {report_orig[target_names[label_to_replace_with]]['recall']}\")"
      ],
      "metadata": {
        "id": "Dx5g6wAbvZuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coyVPop8SuG9"
      },
      "source": [
        "### Look at examples of misclassifications\n",
        "\n",
        "What happened in the two-step model? Did everything improve, or did something get worse? We were concerned about the last news category, which had a very low F1 score in the original model. In the two-step model, the F1 score for that category should have gone up.\n",
        "\n",
        "But for the other category that the original model often confused with the last category, the F1 score might have gone down. In particular, one of the two component metrics, precision or recall, probably went down. (We ask you which one went down in question 3.7 above.)\n",
        "\n",
        "We might be able to tell what happened from the confusion matrix, but it's also always a good idea to look at actual examples that were misclassified, to see if we can spot any patterns. We can also isolate more specific examples, like test examples that the original model got right, but the two-step model got wrong. Let's do that below.\n",
        "\n",
        "**CRITICAL NOTE:**  If nothing prints out when you run the code below, there are two possibilities.  The first is that there is some error in the code or variable names you have created in earlier cells.  The second possibility is that given your current train, validation, and test split, the second model predicted the \"label_to_replace_with\" class and the first model did so too.  This is unlikely but it is possible. In either case, you must go back and re-run the *ENTIRE* notebook to make sure you get a new train, validation, and test split which will allow you to observe the first and second models disagreeing. Please make sure you enter the metric values from this new run into your answers file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj98aBdmNMrE"
      },
      "outputs": [],
      "source": [
        "# Make a vector the length of our test set, with 1 if the second model predicted the\n",
        "# \"label_to_replace_with\" class, and 0s otherwise\n",
        "select_predictions = (predictions_2steps_step3 == label_to_replace_with)\n",
        "select_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaBVzlNXdsWN"
      },
      "outputs": [],
      "source": [
        "# Now only keep a 1 if that was not the correct label, i.e. it was a false positive\n",
        "select_predictions = select_predictions * (test_labels != label_to_replace_with)\n",
        "select_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E05K9v-RdsLT"
      },
      "outputs": [],
      "source": [
        "# And now only keep a 1 if the original model predicted the correct label instead\n",
        "select_predictions = select_predictions * (test_labels == predictions_model1.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4dEwbnc_6qr"
      },
      "outputs": [],
      "source": [
        "# Print out the original and clean text of the examples that met the above conditions\n",
        "for i in np.where(select_predictions)[0]:\n",
        "    \n",
        "    print(f\"Input text number: {i}\")\n",
        "\n",
        "    print('Prediction: model1 = %s, model2 = %s):\\nText: %s\\n\\n' %\n",
        "          (target_names[predictions_model1[i]],\n",
        "           target_names[predictions_2steps_step3[i]],\n",
        "           test_texts[i][:1000].replace('\\n', ' ')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppncPlCzzz0B"
      },
      "source": [
        "**QUESTION:** \n",
        "\n",
        "4.1 Why do you think the two-step model got these examples wrong, when the original model got them right?\n",
        "\n",
        "- A. The two-step model saw less examples of the \"label_to_replace\" class, because we replaced them with the \"label_to_replace_with\" examples. So it didn't learn the kind of text in that class as well as the original model.\n",
        "\n",
        "- B. In the two-step process, the step 1 model overpredicted the combined class, and the step 2 model overpredicted the \"label_to_replace_with\" class. A third class is now getting mistaken more often for the \"label_to_replace_with\" class, than in the original model.\n",
        "\n",
        "- C. It's probably just random that the original model got these specific examples right and the two-step model got them wrong.\n",
        "\n",
        "**B**\n",
        "\n",
        "4.2 Is there anything you might try next, to try to make the two-step model better?\n",
        "\n",
        "- A. Try to balance the training data across classes at each step, or add class weights when calling model.fit.\n",
        "\n",
        "- B. Try to combine another similar category with the two easily confused ones, for a step 1 model with 18 classes and the step 2 model with 3 classes.\n",
        "\n",
        "- C. Try both A and B\n",
        "\n",
        "**C**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}