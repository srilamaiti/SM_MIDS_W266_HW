{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPqd1X9/ytiZY9M+qxIFQfH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/SM_MIDS_W266_HW/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HguVYz4iFCp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import copy\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import re\n",
        "import copy\n",
        "\n",
        "import math\n",
        "from typing import Optional\n",
        "import transformers\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, AdamW, DataCollatorWithPadding\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
        "\n",
        "import pickle\n",
        "import re\n",
        "from transformers import TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification"
      ],
      "metadata": {
        "id": "7_JTYoaMiwhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df['full_text'] = df[\"full_text\"].replace(re.compile(r'[\\n\\r\\t]'), ' ', regex = True)\n",
        "\n",
        "label_cols = df.columns[2:]\n",
        "pred_col_list = ['transformed_pred_' + col for col in label_cols]\n",
        "\n",
        "orig_df = copy.deepcopy(df)\n",
        "orig_df.head()"
      ],
      "metadata": {
        "id": "1MUxch8Ji9Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split = (0.8, 0.2)\n",
        "splits = np.multiply(len(orig_df), split).astype(int)\n",
        "df_train, df_test = orig_df[ : splits[0]], orig_df[splits[0] : ]\n",
        "y_test = np.array(df_test[label_cols], dtype = \"float32\")\n",
        "y_train = np.array(df_train[label_cols], dtype = \"float32\")"
      ],
      "metadata": {
        "id": "yqXJhALvjOmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, TFRobertaModel, RobertaModel\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from transformers import TFAutoModel, AutoModel, AutoTokenizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "ROBERTA_MODEL_CHKPT = \"roberta-base\"\n",
        "BERT_MODEL_CHKPT = \"bert-base-cased\"\n",
        "BERTWEET_MODEL_CHKPT = \"vinai/bertweet-base\"\n",
        "\n",
        "bert_model = BertModel.from_pretrained(BERT_MODEL_CHKPT, output_hidden_states = True)  \n",
        "bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_CHKPT)\n",
        "\n",
        "roberta_model = BertModel.from_pretrained(ROBERTA_MODEL_CHKPT, output_hidden_states = True)  \n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL_CHKPT)\n",
        "\n",
        "bertweet_model = AutoModel.from_pretrained(BERTWEET_MODEL_CHKPT, output_hidden_states = True)\n",
        "bertweet_tokenizer = AutoTokenizer.from_pretrained(BERTWEET_MODEL_CHKPT)"
      ],
      "metadata": {
        "id": "c_irzwtcjSDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.eval()"
      ],
      "metadata": {
        "id": "RWr0gT_v6RLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_model.eval()"
      ],
      "metadata": {
        "id": "3lK3dQ1D6UGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_model.eval()"
      ],
      "metadata": {
        "id": "V0gCI7p56W0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['marked_text'] = \"[CLS] \" + df_train['full_text'] + \" [SEP]\"\n",
        "df_train['marked_text']"
      ],
      "metadata": {
        "id": "ib0lrqnW6wwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "marked_text = df_train['marked_text']"
      ],
      "metadata": {
        "id": "PPl2a9w27Gba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the sentence into tokens.\n",
        "df_train['bert_tokenized_text'] = df_train['marked_text'].apply(lambda x : bert_tokenizer.tokenize(x))\n",
        "df_train['bert_indexed_tokens'] = df_train['bert_tokenized_text'].apply(lambda x : bert_tokenizer.convert_tokens_to_ids(x))\n",
        "\n",
        "df_train['roberta_tokenized_text'] = df_train['marked_text'].apply(lambda x : roberta_tokenizer.tokenize(x))\n",
        "df_train['roberta_indexed_tokens'] = df_train['roberta_tokenized_text'].apply(lambda x : roberta_tokenizer.convert_tokens_to_ids(x))\n",
        "\n",
        "df_train['bertweet_tokenized_text'] = df_train['marked_text'].apply(lambda x : bertweet_tokenizer.tokenize(x))\n",
        "df_train['bertweet_indexed_tokens'] = df_train['bertweet_tokenized_text'].apply(lambda x : bertweet_tokenizer.convert_tokens_to_ids(x))\n"
      ],
      "metadata": {
        "id": "ghmBxH7z7LeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "J9KFGJdx8xuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['bert_segment_id'] = df_train['bert_tokenized_text'].apply(lambda x: [1] * len(x))\n",
        "df_train['roberta_segment_id'] = df_train['roberta_tokenized_text'].apply(lambda x: [1] * len(x))\n",
        "df_train['bertweet_segment_id'] = df_train['bertweet_tokenized_text'].apply(lambda x: [1] * len(x))"
      ],
      "metadata": {
        "id": "jD96X1yQ9FfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.iloc[0]"
      ],
      "metadata": {
        "id": "Dm2WWU-T9e0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "bert_tokens_tensor = torch.tensor([df_train['bert_indexed_tokens'].iloc[0]])\n",
        "bert_segments_tensor = torch.tensor(df_train['bert_segment_id'].iloc[0])\n",
        "'''\n",
        "roberta_tokens_tensor = torch.tensor([df_train['roberta_indexed_tokens']])\n",
        "roberta_segments_tensors = torch.tensor(df_train['roberta_segment_id'])\n",
        "\n",
        "bertweet_tokens_tensor = torch.tensor([df_train['bertweet_indexed_tokens']])\n",
        "bertweet_segments_tensors = torch.tensor(df_train['bertweet_segment_id'])\n",
        "'''"
      ],
      "metadata": {
        "id": "Nn_U-cVZ9rDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_segments_tensor"
      ],
      "metadata": {
        "id": "mGfgZZMu_GCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokens_tensor"
      ],
      "metadata": {
        "id": "cJ2fGIpH_DRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the text through BERT, and collect all of the hidden states produced\n",
        "# from all 12 layers. \n",
        "with torch.no_grad():\n",
        "\n",
        "    outputs = bert_model(bert_tokens_tensor, bert_segments_tensor)\n",
        "    print(outputs)\n",
        "    # Evaluating the model will return a different number of objects based on \n",
        "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
        "    # becase we set `output_hidden_states = True`, the third item will be the \n",
        "    # hidden states from all layers. See the documentation for more details:\n",
        "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    hidden_states = outputs[2]"
      ],
      "metadata": {
        "id": "iYtKMjcW6ixz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.config"
      ],
      "metadata": {
        "id": "6wj1JX4nkjTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_model.config"
      ],
      "metadata": {
        "id": "v-jUwJaSks1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertweet_model.config"
      ],
      "metadata": {
        "id": "VHERCkKfkvVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "learning_rate = 1e-5\n",
        "dropout = .1\n",
        "epochs = 1\n",
        "batch_size = 4\n",
        "input_ids = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'input_ids')\n",
        "attention_masks = tf.keras.Input(shape = (MAX_LEN ,), dtype = 'int64', name = 'attention_masks')"
      ],
      "metadata": {
        "id": "zt1QrX0njKy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_encode(texts, tokenizer, max_len):\n",
        "    input_ids = []\n",
        "    # token_type_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for text in texts:\n",
        "        token = tokenizer(text, \n",
        "                          max_length = max_len, \n",
        "                          truncation = True, \n",
        "                          padding = 'max_length',\n",
        "                          add_special_tokens = True)\n",
        "        input_ids.append(token['input_ids'])\n",
        "        # token_type_ids.append(token['token_type_ids'])\n",
        "        attention_mask.append(token['attention_mask'])\n",
        "    \n",
        "    return np.array(input_ids), np.array(attention_mask)"
      ],
      "metadata": {
        "id": "EVv15rPfltaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MCRMSE(y_true, y_pred):\n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis = 1)\n",
        "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis = -1, keepdims = True)"
      ],
      "metadata": {
        "id": "n_nDVx1yjK11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_bert_transformer = bert_model([input_ids, attention_masks])\n",
        "pretrained_roberta_transformer = roberta_model([input_ids, attention_masks])\n",
        "pretrained_bertweet_transformer = bertweet_model([input_ids, attention_masks])"
      ],
      "metadata": {
        "id": "QDQiVOrwjK4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pretrained_bert_transformer), len(pretrained_roberta_transformer), len(pretrained_bertweet_transformer)"
      ],
      "metadata": {
        "id": "gKVlTlFCjK_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_bert_transformer.keys(), pretrained_roberta_transformer.keys(), pretrained_bertweet_transformer.keys()"
      ],
      "metadata": {
        "id": "xoK25ylBmisZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_last_hidden_state = pretrained_bert_transformer['last_hidden_state']\n",
        "roberta_last_hidden_state = pretrained_roberta_transformer['last_hidden_state']\n",
        "bertweet_last_hidden_state = pretrained_bertweet_transformer['last_hidden_state']\n",
        "\n",
        "bert_pooler_output = pretrained_bert_transformer['pooler_output']\n",
        "roberta_last_hidden_state = pretrained_roberta_transformer['pooler_output']\n",
        "bertweet_last_hidden_state = pretrained_bertweet_transformer['pooler_output']\n",
        "\n",
        "bert_attentions = pretrained_bert_transformer['attentions']\n",
        "roberta_attentions = pretrained_roberta_transformer['attentions']\n",
        "bertweet_attentions = pretrained_bertweet_transformer['attentions']\n",
        "\n",
        "cls_token_bert = pretrained_bert_transformer[0][:, 0, :]\n",
        "cls_token_roberta = pretrained_roberta_transformer[0][:, 0, :]\n",
        "cls_token_bertweet = pretrained_bertweet_transformer[0][:, 0, :]"
      ],
      "metadata": {
        "id": "Su2nEamamteb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_bert = tf.keras.layers.Dense(6)(cls_token_bert)\n",
        "bertmodel = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = output_bert)\n",
        "bertmodel.compile(tf.keras.optimizers.Adam(learning_rate), loss = MCRMSE, metrics = MCRMSE)\n",
        "\n",
        "output_roberta = tf.keras.layers.Dense(6)(cls_token_roberta)\n",
        "robertamodel = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = cls_token_roberta)\n",
        "robertamodel.compile(tf.keras.optimizers.Adam(learning_rate), loss = MCRMSE, metrics = MCRMSE)\n",
        "\n",
        "output_bertweet = tf.keras.layers.Dense(6)(cls_token_bertweet)\n",
        "bertweetmodel = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs = cls_token_bertweet)\n",
        "bertweetmodel.compile(tf.keras.optimizers.Adam(learning_rate), loss = MCRMSE, metrics = MCRMSE)"
      ],
      "metadata": {
        "id": "DrlRZvyBrp55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoded_input_ids_bert, train_encoded_attention_masks_bert = text_encode(df_train['full_text'], bert_tokenizer, MAX_LEN)\n",
        "train_encoded_input_ids_roberta, train_encoded_attention_masks_roberta = text_encode(df_train['full_text'], roberta_tokenizer, MAX_LEN)\n",
        "train_encoded_input_ids_bertweet, train_encoded_attention_masks_bertweet = text_encode(df_train['full_text'], bertweet_tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "IxPdkf6msU1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                  loss      = MCRMSE,\n",
        "                  metrics   = MCRMSE\n",
        "                 )\n",
        "\n",
        "robertamodel.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                  loss      = MCRMSE,\n",
        "                  metrics   = MCRMSE\n",
        "                 )\n",
        "\n",
        "bertweetmodel.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "                  loss      = MCRMSE,\n",
        "                  metrics   = MCRMSE\n",
        "                 )"
      ],
      "metadata": {
        "id": "nGrLLzzxsUx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "berthist = bertmodel.fit((train_encoded_input_ids_bert, train_encoded_attention_masks_bert),\n",
        "                     y_train,\n",
        "                     validation_split = .1,\n",
        "                     batch_size = batch_size,        \n",
        "                     epochs = epochs\n",
        "                    )\n",
        "\n",
        "robertamodelhist = robertamodel.fit((train_encoded_input_ids_roberta, train_encoded_attention_masks_roberta),\n",
        "                     y_train,\n",
        "                     validation_split = .1,\n",
        "                     batch_size = batch_size,        \n",
        "                     epochs = epochs\n",
        "                    )\n",
        "\n",
        "bertweethist = bertweetmodel.fit((train_encoded_input_ids_bertweet, \n",
        "                                  train_encoded_attention_masks_bertweet\n",
        "                                 ),\n",
        "                     y_train,\n",
        "                     validation_split = .1,\n",
        "                     batch_size = batch_size,        \n",
        "                     epochs = epochs\n",
        "                    )"
      ],
      "metadata": {
        "id": "QNzQ5_WosUut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqOOn-H7sUr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9tCExkcsUpB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}